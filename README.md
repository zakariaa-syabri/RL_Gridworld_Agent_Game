# GridWorld Q-Learning - Apprentissage par Renforcement

Projet de **Reinforcement Learning** avec un environnement GridWorld interactif. L'agent utilise **Q-Learning** pour apprendre par **exp√©rience** (essais/erreurs) et non par planification.

##  Caract√©ristiques principales

- ‚úÖ **Apprentissage R√âEL** : Q-Learning au lieu de Value Iteration (l'agent apprend vraiment)
- ‚úÖ **Visualisation en temps r√©el** : Regardez l'agent apprendre √©pisode par √©pisode
- ‚úÖ **Configuration flexible** : Presets ou configuration personnalis√©e compl√®te
- ‚úÖ **Goal dynamique** : Le goal change √† chaque √©pisode pour tester l'adaptabilit√©
- ‚úÖ **G√©n√©ration al√©atoire** : Obstacles g√©n√©r√©s automatiquement
- ‚úÖ **Courbes d'apprentissage** : Visualisez la progression de l'agent
- ‚úÖ **Interface interactive** : Menu simple et intuitif

##  Structure du Projet

```
RL_game/
‚îú‚îÄ‚îÄ gridworld_env.py           # Environnement GridWorld (inspir√© de Gymnasium)
‚îú‚îÄ‚îÄ q_learning_agent.py        # Agent Q-Learning (apprentissage r√©el)
‚îú‚îÄ‚îÄ value_iteration_agent.py   # Agents Random et Value Iteration (optionnel)
‚îú‚îÄ‚îÄ environment_setup.py       # Configuration interactive et presets
‚îú‚îÄ‚îÄ config.py                  # Configuration par d√©faut
‚îú‚îÄ‚îÄ main.py                    # Programme principal
‚îú‚îÄ‚îÄ README.md                  # Ce fichier
‚îî‚îÄ‚îÄ GUIDE_UTILISATION.md       # Guide d√©taill√©
```

##  Installation

### Pr√©requis
- Python 3.7+
- NumPy
- Matplotlib

### Installation des d√©pendances

```bash
pip install numpy matplotlib
```

##  Utilisation

### D√©marrage rapide

```bash
python main.py
```

### Menu de configuration

Au d√©marrage, 3 options s'offrent √† vous :

#### 1Ô∏è‚É£ Configuration par d√©faut
- Grille : 6x6
- Obstacles : 6 (pr√©d√©finis)
- D√©marrage imm√©diat

#### 2Ô∏è‚É£ Presets rapides
- **Petit** (5x5, 3 obstacles) ‚Üí Facile, apprentissage rapide
- **Moyen** (8x8, 8 obstacles) ‚Üí Interm√©diaire, √©quilibr√©
- **Grand** (10x10, 15 obstacles) ‚Üí Difficile, plus de complexit√©
- **Tr√®s grand** (15x15, 30 obstacles) ‚Üí Tr√®s difficile, maximum challenge

#### 3Ô∏è‚É£ Configuration personnalis√©e
Choisissez :
- Dimensions de la grille (2-20 lignes/colonnes)
- Nombre d'obstacles (avec validation automatique)
- Position de d√©part
- Position du goal
- G√©n√©ration al√©atoire des obstacles

## Ce que vous allez voir

### 1. Test de l'agent random (baseline)
Performance de base avec actions al√©atoires

### 2. Entra√Ænement Q-Learning visualis√©

**Tous les 50 √©pisodes**, vous verrez :
- L'agent se d√©placer dans la grille
- Les Q-values √©voluer en temps r√©el
- Mode EXPLORATION ‚Üí EXPLOITATION
- Epsilon d√©cro√Ætre (100% ‚Üí 1%)
- Trajectoires s'am√©liorer

**Progression typique** :
```
√âpisode 50  : Exploration pure (mouvements al√©atoires)
√âpisode 100 : Commence √† apprendre des patterns
√âpisode 200 : Trajectoires plus efficaces
√âpisode 300 : Performance quasi-optimale
```

### 3. Courbes d'apprentissage

4 graphiques montrant :
- **R√©compenses** : √âvolution des r√©compenses par √©pisode
- **Steps** : Nombre de pas par √©pisode (d√©cro√Æt)
- **Epsilon** : D√©croissance de l'exploration
- **Taux de succ√®s** : Pourcentage de r√©ussite

### 4. Animation avec goal dynamique

L'agent s'adapte √† de nouveaux goals :
- Goal change √† chaque √©pisode
- L'agent recalcule rapidement
- D√©montre la g√©n√©ralisation

### 5. Visualisation finale

Grille avec :
- **Q-values** : Valeurs optimales de chaque √©tat
- **Politique** : Fl√®ches indiquant les meilleures actions
- **Couleurs** : Intensit√© selon la valeur
- **Agent** : Cercle rouge se d√©pla√ßant

##  Configuration

### Fichier `config.py`

```python
# Dimensions (si mode par d√©faut)
GRID_SIZE = (6, 6)
START_POS = (0, 0)
GOAL_POS = (5, 5)
OBSTACLES = [(1,1), (1,2), (2,3), (3,3), (4,1), (4,2)]

# Q-Learning
USE_Q_LEARNING = True          # Active Q-Learning
LEARNING_RATE = 0.1            # Vitesse d'apprentissage
EPSILON_START = 1.0            # Exploration initiale (100%)
EPSILON_DECAY = 0.995          # D√©croissance d'epsilon
EPSILON_MIN = 0.01             # Exploration minimale (1%)
NUM_TRAINING_EPISODES = 300    # Nombre d'√©pisodes d'entra√Ænement

# Visualisation de l'entra√Ænement
VISUALIZE_TRAINING = True      # Voir l'agent apprendre
VISUALIZE_EVERY = 50           # Visualiser tous les N √©pisodes
TRAINING_ANIMATION_DELAY = 0.1 # Vitesse de l'animation
SHOW_Q_VALUES_TRAINING = True  # Afficher les Q-values

# Goal dynamique
DYNAMIC_GOAL = True            # Goal change √† chaque √©pisode
NUM_ANIMATED_EPISODES = 3      # Nombre d'√©pisodes √† animer
```

### Ajuster selon la taille de la grille

**Petite grille (5x5)** :
```python
NUM_TRAINING_EPISODES = 150
VISUALIZE_EVERY = 30
```

**Moyenne grille (8x8)** :
```python
NUM_TRAINING_EPISODES = 300
VISUALIZE_EVERY = 50
```

**Grande grille (10x10+)** :
```python
NUM_TRAINING_EPISODES = 500
VISUALIZE_EVERY = 100
```

##  Algorithme Q-Learning

Q-Learning est un algorithme d'**apprentissage par renforcement** sans mod√®le (model-free).

### R√®gle de mise √† jour

```
Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_a' Q(s',a') - Q(s,a)]
```

O√π :
- **Q(s,a)** : Valeur Q de l'√©tat s avec l'action a
- **Œ±** : Learning rate (vitesse d'apprentissage)
- **r** : R√©compense imm√©diate
- **Œ≥** : Gamma (discount factor)
- **s'** : √âtat suivant
- **max_a' Q(s',a')** : Meilleure Q-value du prochain √©tat

### Exploration vs Exploitation (Œµ-greedy)

L'agent √©quilibre :
- **Exploration** : Essayer de nouvelles actions (Œµ = epsilon)
- **Exploitation** : Utiliser les meilleures actions connues (1-Œµ)

```
D√©but : Œµ = 100% ‚Üí Exploration pure
Fin   : Œµ = 1%   ‚Üí Exploitation quasi-pure
```

##  Exemple de sortie

```
=======================================================================
     GRIDWORLD - REINFORCEMENT LEARNING avec Q-Learning
=======================================================================

üìã MODE DE CONFIGURATION
-----------------------------------------------------------------------
1. Configuration par d√©faut (fichier config.py)
2. Presets rapides (Petit/Moyen/Grand/Tr√®s grand)
3. Configuration personnalis√©e (Interactive)
-----------------------------------------------------------------------
Votre choix (1-3, d√©faut: 1): 2

=======================================================================
CONFIGURATION RAPIDE - PRESETS
=======================================================================
1. Petit (5x5, 3 obstacles) - Facile
2. Moyen (8x8, 8 obstacles) - Interm√©diaire
3. Grand (10x10, 15 obstacles) - Difficile
4. Tr√®s grand (15x15, 30 obstacles) - Tr√®s difficile
5. Configuration personnalis√©e
=======================================================================
Votre choix (1-5, d√©faut: 2): 2

‚úì Configuration: Grille 8x8, 8 obstacles

=======================================================================
ENVIRONNEMENT CR√â√â
=======================================================================
Taille de la grille: (8, 8)
Position de d√©part: (0, 0)
Position du goal: (7, 7)
Nombre d'obstacles: 8
Cellules libres: 56
=======================================================================

==================================================
ENTRA√éNEMENT PAR Q-LEARNING (APPRENTISSAGE R√âEL)
==================================================
L'agent va APPRENDRE par essais/erreurs

Param√®tres:
  - Learning rate (alpha): 0.1
  - Gamma (discount): 0.95
  - Epsilon (exploration): 1.0 ‚Üí 0.01
  - Nombre d'√©pisodes: 300

D√©but de l'entra√Ænement sur 300 √©pisodes...
Visualisation activ√©e tous les 50 √©pisodes

>>> Visualisation de l'√©pisode 50...
  [Animation de l'agent explorant la grille]

√âpisode 50/300 | R√©compense moy: -0.450 | Steps moy: 45.2 | Succ√®s: 12.0% | Epsilon: 0.778

>>> Visualisation de l'√©pisode 100...
  [Animation avec trajectoires plus efficaces]

√âpisode 100/300 | R√©compense moy: 0.234 | Steps moy: 28.5 | Succ√®s: 58.0% | Epsilon: 0.605

>>> Visualisation de l'√©pisode 150...
  [Animation avec trajectoires quasi-optimales]

√âpisode 150/300 | R√©compense moy: 0.678 | Steps moy: 15.2 | Succ√®s: 88.0% | Epsilon: 0.471

...

Entra√Ænement termin√©!

==================================================
COURBES D'APPRENTISSAGE
==================================================
[Affichage des 4 graphiques de progression]

==================================================
TEST DE L'AGENT Q-LEARNING ENTRA√éN√â
==================================================
Test en mode exploitation (epsilon = 0)...

√âpisode 1: R√©compense = 0.891, Steps = 12 ‚úì
√âpisode 2: R√©compense = 0.891, Steps = 12 ‚úì
√âpisode 3: R√©compense = 0.891, Steps = 12 ‚úì
√âpisode 4: R√©compense = 0.891, Steps = 12 ‚úì
√âpisode 5: R√©compense = 0.891, Steps = 12 ‚úì

R√©compense moyenne: 0.891
Taux de succ√®s: 100.0%

==================================================
ANIMATION DES √âPISODES
==================================================
Animation de 3 √©pisodes avec GOAL DYNAMIQUE...
Le goal change √† chaque √©pisode pour tester l'adaptabilit√© de l'agent.

√âpisode anim√© 1/3...
  Nouveau goal: (3, 7)
  R√©entra√Ænement pour le nouveau goal...
  [L'agent s'adapte au nouveau goal]
  ‚Üí Termin√©: 8 steps, r√©compense = 0.921 ‚úì

...
```

##  Fonctionnalit√©s avanc√©es

### Goal dynamique

Avec `DYNAMIC_GOAL = True`, le goal change √† chaque √©pisode :
- Teste la **g√©n√©ralisation** de l'agent
- Prouve que l'agent comprend la **structure** du gridworld
- Pas juste m√©morisation, mais **vraie compr√©hension**

### G√©n√©ration al√©atoire d'obstacles

Les obstacles sont g√©n√©r√©s automatiquement :
- Distribution al√©atoire dans la grille
- √âvite automatiquement start et goal
- Garantit un chemin possible

### Validation automatique

Le syst√®me valide :
- Grille minimale : 2x2
- Au moins 3 cellules libres (start + goal + chemin)
- Start ‚â† Goal
- Nombre d'obstacles valide

##  Fichiers principaux

### `gridworld_env.py`
Environnement GridWorld (inspir√© de Gymnasium) :
- M√©thodes : `reset()`, `step()`, `get_next_state()`, etc.
- 4 actions : UP, DOWN, LEFT, RIGHT
- Syst√®me de r√©compenses
- Support goal dynamique

### `q_learning_agent.py`
Agent Q-Learning :
- Apprentissage par exp√©rience
- Exploration epsilon-greedy
- Mise √† jour des Q-values
- Extraction de la politique optimale

### `environment_setup.py`
Configuration interactive :
- Presets rapides
- Configuration personnalis√©e
- G√©n√©ration d'obstacles
- Validation

### `main.py`
Programme principal :
- Menu de configuration
- Entra√Ænement visualis√©
- Courbes d'apprentissage
- Animations

##  Conseils d'utilisation

### Pour d√©buter
1. Utilisez le preset **Petit** (option 2 ‚Üí 1)
2. Observez l'entra√Ænement tous les 30 √©pisodes
3. Notez comment les trajectoires s'am√©liorent

### Pour exp√©rimenter
1. Cr√©ez une configuration personnalis√©e (option 3)
2. Testez diff√©rentes tailles et densit√©s d'obstacles
3. Ajustez `NUM_TRAINING_EPISODES` selon la complexit√©

### Pour comprendre Q-Learning
1. Regardez les Q-values √©voluer pendant l'entra√Ænement
2. Observez epsilon d√©cro√Ætre (exploration ‚Üí exploitation)
3. Comparez les courbes de r√©compenses et steps

##  Roncontre des probl√©matiques

**L'agent n'apprend pas** :
- Augmenter `NUM_TRAINING_EPISODES`
- V√©rifier qu'il y a un chemin vers le goal
- Ajuster `LEARNING_RATE` (essayer 0.05 ou 0.2)

**Visualisation trop lente** :
- Augmenter `VISUALIZE_EVERY` (ex: 100)
- R√©duire `TRAINING_ANIMATION_DELAY` (ex: 0.05)
- D√©sactiver `VISUALIZE_TRAINING` temporairement

**Grille trop grande** :
- Augmenter `NUM_TRAINING_EPISODES` proportionnellement
- Pour 15x15 : au moins 500 √©pisodes recommand√©s


## üìÑ Licence

Projet √©ducatif de d√©monstration pour l'apprentissage du Reinforcement Learning.

##  Auteur

Projet cr√©√© pour illustrer les concepts de Reinforcement Learning avec Q-Learning.
SYABRI Zakariaa
---


